table_wiki1 <- html_table(Section_wikipedia1)
table_wiki1 <- html_text(Section_wikipedia1)
table_wiki1
Section_wikipedia1 <- html_node(wikipedia_page, css = '.jquery-tablesorter td , .headerSort')
Section_wikipedia1
table_wiki1 <- html_text(Section_wikipedia1)
Section_wikipedia1 <- html_node(wikipedia_page, css = '.jquery-tablesorter td')
wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
Section_wikipedia1 <- html_node(wikipedia_page, css = '.jquery-tablesorter td')
Section_wikipedia1 <- html_node(wikipedia_page, xpath = '.jquery-tablesorter td')
Section_wikipedia1 <- html_node(wikipedia_page, xpath = '.jquery-tablesorter td')
Section_wikipedia1 <- html_node(wikipedia_page, css = '.jquery-tablesorter td+ td , .flagicon+ a , .headerSort')
Section_wikipedia1
table_wiki1 <- html_text(Section_wikipedia1)
table_wiki1
Section_wikipedia1 <- html_node(wikipedia_page, css = '.headerSort , .jquery-tablesorter td')
#mw-content-text > div.mw-parser-output > table
#mw-content-text > div.mw-parser-output > table
Section_wikipedia1 <- html_node(wikipedia_page, css = '#mw-content-text > div.mw-parser-output > table')
table_wiki1 <- html_text(Section_wikipedia1)
table_wiki1
table_wiki1 <- html_table(Section_wikipedia1)
table_wiki1
#.jquery-tablesorter td , .headerSort
Section_wikipedia1 <- html_node(wikipedia_page, css = '.jquery-tablesorter td , .headerSort')
lego_section <- html_node(lego_film, css = '.primary_photo+ td a')
lego_film <- read_html("https://www.imdb.com/title/tt1490017/")
lego_section <- html_node(lego_film, css = '.primary_photo+ td a')
lego_section
Nom_acteur <- html_table(lego_section)
Nom_acteur <- html_text(lego_section)
head(Nom_acteur)
lego_section <- html_node(lego_film, css = '.odd td')
lego_section
Nom_acteur <- html_table(lego_section)
Nom_acteur <- html_text(lego_section)
head(Nom_acteur)
lego_section <- html_node(lego_film, css = '.primary_photo+ td a , .primary_photo+ td')
lego_section
Nom_acteur <- html_text(lego_section)
head(Nom_acteur)
acteurs <-
lego %>%
html_nodes(css = ".primary_photo+ td a")
lego <- read_html("https://www.imdb.com/title/tt1490017/")
acteurs <-
lego %>%
html_nodes(css = ".primary_photo+ td a")
acteurs <-
lego %>%
html_node(css = ".primary_photo+ td a")
acteurs <-
lego %>%
html_node(css = ".primary_photo+ td a")
acteurs <-
lego %>%
html_node(css = '.primary_photo+ td a')
acteurs
acteurs <-
lego %>%
html_node(css = '.primary_photo+ td')
acteurs <-
lego %>%
html_node(css = '.primary_photo+ td')
lego <- read_html("https://www.imdb.com/title/tt1490017/")
aacteurs <- html_node(lego, css = '.primary_photo+ td')
length(acteurs)
acteurs <- html_node(lego, css = '.primary_photo+ td')
length(acteurs)
acteurs[1:2]
# Collecter les noms
acteurs_nom <- html_text(acteurs, trim = TRUE)
# Collecter les liens des pages des acteurs
acteur_attr <- html_attrs(acteurs)
# Relative url
acteurs_rel_url <- html_attr(acteurs, "href")
acteurs <- html_node(lego, xpath = '.primary_photo+ td')
acteurs <- html_node(lego, xpath = '.primary_photo+ td')
acteurs <- html_nodes(lego, xpath = '.primary_photo+ td')
acteurs <- html_nodes(lego, xpath = '.primary_photo+ td a')
acteurs <-
lego %>%
html_node(css = '.primary_photo+ td a')
#.orange a span
thinkr_formateurs_nodes <- html_node(thinkr_url, css = '.orange , .orange span')
thinkr_url <- read_html("https://thinkr.fr/equipe/")
#.orange a span
thinkr_formateurs_nodes <- html_node(thinkr_url, css = '.orange , .orange span')
length(thinkr_formateurs_nodes)
thinkr_formateurs_nodes[1:3]
formateur <- html_text(thinkr_formateurs_nodes)
length(formateur)
formateur[3]
length(formateur)
#.orange a span
thinkr_formateurs_nodes <- html_node(thinkr_url, css = '.orange span')
length(thinkr_formateurs_nodes)
thinkr_formateurs_nodes[1:3]
formateur <- html_text(thinkr_formateurs_nodes)
length(formateur)
formateur[3]
formateur[1]
formateur[2]
sociouqam_url <- read_html("https://sociologie.uqam.ca/corps-professoral/professeurs-es/")
profs <-
sociouqam_url %>%
html_nodes('.nom a') %>%
html_text(trim = TRUE)
head(profs)
profs[1]
profs[2]
prof_table <- as_data_frame(c(profs[1:35]))
prof_table
nyt_nodes <-
nyt_url %>%
html_nodes('.css-aa7djq :nth-child(1)')
nyt_url <- read_html("https://www.nytimes.com/2019/11/28/opinion/thanksgiving-trump.html#commentsContainer")
nyt_nodes <-
nyt_url %>%
html_nodes('.css-aa7djq :nth-child(1)')
lenght(nyt_nodes)
nyt_nodes <-
nyt_url %>%
html_nodes('//*[@id="comment-content-1"]')
system('docker run -d -p 4445:4444 selenium/standalone-chrome')
#install.packages("RSelenium")
library(RSelenium)
# start a Selenium server
rD <- rsDriver(browser = c("chrome"), chromever = "78.0.3904.105")
# open the browser
remDr <- rD$client
remDr$navigate("https://www.duke.edu")
remDr$open("https://www.duke.edu)
remDr$open("https://www.duke.edu")
remDr$open()
remDr$navigate("https://www.duke.edu")
remDr$navigate("https://www.duke.edu")
search_box <- remDr$findElement(using = 'css selector', 'fieldset input')
search_box$sendKeysToElement(list("data science", "\uE007"))
remDr$close()
rD$server$stop()
# start a Selenium server
rD <- rsDriver(browser = c("chrome"), chromever = "78.0.3904.105")
# open the browser
remDr <- rD$client
# open the browser
remDr <- rD$client
remDr$navigate("https://www.duke.edu")
remDr$open()
rm(list = ls())
library(tidyverse)
#install.packages("tidystopwords")
library(tidyverse)
library(readxl)
library(lubridate)
library(summarytools)
library(gtsummary)
library(tidytext)
library(tidystopwords)  # Stop word
fbcomments <- read_csv("FBComment_updateC.csv")
fbposts <- read_csv("FBPost_updateC.csv")
fbposts <- read_csv("FBPost_updateC.csv")
fbposts <- read_delim("FBPost_updateC.csv", delim = "\t"))
fbposts <- read_delim("FBPost_updateC.csv", delim = "\t")
View(fbposts)
library(tidyverse)
library(summarytools)
restaurants <- read_csv("../Donnees/restaurants.csv",
col_types = "ccccccccnnccicccciccciD")
glimpse(restaurants)
head(restaurants$Zip_Code)
restaurants <-
restaurants %>%
mutate(ZIP_length = nchar(Zip_Code)) #%>%
freq(restaurants$ZIP_length)
restaurants <-
restaurants %>%
mutate(ZIP_5 = substr(Zip_Code, 1, 5),
ZIP_3 = substr(Zip_Code, 2, 4))
restaurants %>% distinct(ZIP_5) %>% head()
restaurants %>% distinct(ZIP_3) %>% head()
restaurants %>% distinct(ZIP_5) %>% head()
restaurants %>% distinct(ZIP_3) %>% head()
restaurants <-
restaurants %>%
mutate(mailing_address =
paste(Address, ", ", City, ", WA ", ZIP_5, sep = ""))
restaurants %>% distinct(mailing_address) %>% head()
paste(1:5, letters[1:5]) # sep met un espace par défaut
paste(1:5, letters[1:6]) # sep met un espace par défaut
paste(1:5, letters[1:5]) # sep met un espace par défaut
paste0(1:5, letters[1:5])
paste(1:5, letters[1:5], sep ="")
paste(1:5, letters[1:5]) # sep met un espace par défaut
paste(letters[1:5])
paste(1:5, letters[1:5], sep = "+")
paste(letters[1:5], collapse = "!")
paste(letters[1:5], sep = "+")
paste(letters[1:5], collapse = "!")
paste0(1:5, letters[1:5], collapse = "???")
?paste
paste(1:5, letters[1:5], sep = "+")
paste(1:5, "Z", sep = "*", collapse = " ~ ")
str_sub("Washington", 3, 8)
str_sub("Washington", -3, -1)
str_sub("Washington", -3, -5)
str_sub("Washington", -5, -3)
str_sub("Washington", 8, 3)
str_sub("Washington", 8)
str_sub("Washington", 8, 10)
str_sub("Washington", 8, 9)
letters
1:5
str_c(letters[1:5], 1:5)
str_c(letters[1:5], 1:5)
nchar("voisines")
str_length("voisines")
head(restaurants$City)
head(unique(restaurants$Name))
head(unique(restaurants$Address))
head(unique(restaurants$Name))
head(unique(restaurants$City))
restaurants <-
restaurants %>%
mutate_at(vars(Name, Address, City), ~ str_to_upper(.))
head(unique(restaurants$City))
restaurants <-
restaurants %>%
mutate(Name = str_to_upper(),
Address = str_to_upper(),
City = str_to_upper())
restaurants <-
restaurants %>%
mutate(Name = str_to_upper(Name),
Address = str_to_upper(Address),
City = str_to_upper(City))
head(unique(restaurants$Name), 20)
restaurants <-
restaurants %>%
mutate_if(is.character, str_trim)
head(unique(restaurants$Name), 4)
?"'"
coffee <-
restaurants %>%
filter(str_detect(Name, "COFFEE|ESPRESSO|ROASTER|CAFE"))
coffee1 <-
restaurants %>%
filter(grepl("COFFEE|ESPRESSO|ROASTER", Name))
View(coffee)
View(coffee1)
View(restaurants)
View(coffee)
coffee1 <-
restaurants %>%
filter(grepl("COFFEE|ESPRESSO|ROASTER|CAFE", Name))
code_206_pattern <- "^\\(?206|^\\(?306"
exemple_test <- c("2061234567", "(206)1234567",
"(306) 123-4567", "555-206-1234")
exemple_test
str_detect(exemple_test, "^\\(?206")
str_detect(exemple_test, code_206_pattern)
exemple_test
str_detect(exemple_test, "206")
str_view(exemple_test, code_206_pattern)
code_206_pattern <- "^\\(?206"
restaurants %>%
mutate(contient_nombre_206 = str_detect(Phone, code_206_pattern)) %>%
group_by(contient_nombre_206) %>%
tally()
?tally
restaurants %>%
mutate(contient_nombre_206 = str_detect(Phone, code_206_pattern)) %>%
group_by(contient_nombre_206) %>%
count()
restaurants %>%
mutate(contient_nombre_206 = str_detect(Phone, code_206_pattern)) %>%
group_by(contient_nombre_206) %>%
tally()
View(restaurants)
restaurants %>%
mutate(contient_nombre_206 = str_detect(Phone, code_206_pattern))
direction_pattern <- " (N|NW|NE|S|SW|SE|W|E)( |$)"
direction_pattern1 <- "(N|NW|NE|S|SW|SE|W|E)"
direction_exemples <- c("2812 THORNDYKE AVE W", "512 NW 65TH ST",
"407 CEDAR ST", "15 NICKERSON ST ")
direction_exemples
str_extract(direction_exemples, "N")
str_extract(direction_exemples, direction_pattern1)
str_extract(direction_exemples, direction_pattern)
restaurants %>%
distinct(Address) %>%
mutate(city_region =
str_trim(str_extract(Address, direction_pattern))) %>%
count(city_region) %>% arrange(desc(n))
restaurants %>%
# distinct(Address) %>%
mutate(city_region =
str_trim(str_extract(Address, direction_pattern))) %>%
count(city_region) %>% arrange(desc(n))
View(restaurants)
restaurants %>%
distinct(Address) %>%
mutate(city_region =
str_trim(str_extract(Address, direction_pattern))) %>%
count(city_region) %>% arrange(desc(n))
restaurants %>%
unique(Address) %>%
mutate(city_region =
str_trim(str_extract(Address, direction_pattern))) %>%
count(city_region) %>% arrange(desc(n))
restaurants %>%
unique(Address) %>%
mutate(city_region =
str_trim(str_extract(Address, direction_pattern))) %>%
count(city_region) %>% arrange(desc(n))
restaurants %>%
distinct(Address) %>%
mutate(city_region =
str_trim(str_extract(Address, direction_pattern))) %>%
count(city_region) %>% arrange(desc(n))
restaurants %>%
distinct(Address) %>%
mutate(city_region = str_extract(Address, direction_pattern)) %>%
#str_trim(str_extract(Address, direction_pattern))) %>%
count(city_region) %>% arrange(desc(n))
restaurants <-
restaurants %>%
distinct(Address) %>%
mutate(city_region = str_extract(Address, direction_pattern))
View(restaurants)
restaurants <- read_csv("../Donnees/restaurants.csv",
col_types = "ccccccccnnccicccciccciD")
restaurants <-
restaurants %>%
distinct(Address) %>%
mutate(city_region = #str_extract(Address, direction_pattern)) %>%
str_trim(str_extract(Address, direction_pattern)))
View(restaurants)
restaurants %>%
distinct(Address) %>%
mutate(city_region = #str_extract(Address, direction_pattern)) %>%
str_trim(str_extract(Address, direction_pattern))) %>%
count(city_region) %>% arrange(desc(n))
restaurants <- read_csv("../Donnees/restaurants.csv",
col_types = "ccccccccnnccicccciccciD")
#restaurants <-
restaurants %>%
distinct(Address) %>%
mutate(city_region = #str_extract(Address, direction_pattern)) %>%
str_trim(str_extract(Address, direction_pattern))) %>%
count(city_region) %>% arrange(desc(n))
address_number_pattern <- "^[0-9]*-?[A-Z]? (1/2 )?"
address_number_test_examples <-
c("2812 THORNDYKE AVE W", "1ST AVE", "10AA 1ST AVE",
"10-A 1ST AVE", "5201-B UNIVERSITY WAY NE",
"7040 1/2 15TH AVE NW")
address_number_test_examples
str_replace(address_number_test_examples,
address_number_pattern, replacement = "")
address_number_pattern <- "^[0-9]*-?[A-Z]*? (1/2 )?"
address_number_test_examples <-
c("2812 THORNDYKE AVE W", "1ST AVE", "10AA 1ST AVE",
"10-A 1ST AVE", "5201-B UNIVERSITY WAY NE",
"7040 1/2 15TH AVE NW")
address_number_test_examples
str_replace(address_number_test_examples,
address_number_pattern, replacement = "")
address_number_pattern <- "^[0-9]*-?[A-Z]? (1/2 )?"
address_number_test_examples <-
c("2812 THORNDYKE AVE W", "1ST AVE", "10AA 1ST AVE",
"10-A 1ST AVE", "5201-B UNIVERSITY WAY NE",
"7040 1/2 15TH AVE NW")
address_number_test_examples
str_replace(address_number_test_examples,
address_number_pattern, replacement = "")
restaurants <-
restaurants %>%
mutate(street_only = str_replace(Address, address_number_pattern,
replacement = ""))
restaurants %>% distinct(street_only) %>% head(11)
address_unit_pattern <- " (#|STE|SUITE|SHOP|UNIT).*$"
address_unit_test_examples <-
c("1ST AVE", "RAINIER AVE S #A", "FAUNTLEROY WAY SW STE 108",
"4TH AVE #100C", "NW 54TH ST")
str_replace(address_unit_test_examples, address_unit_pattern,
replacement = "")
restaurants <-
restaurants %>%
mutate(street_only = str_trim(str_replace(street_only,
address_unit_pattern, replacement = "")))
restaurants %>% distinct(street_only) %>% head(11)
address_unit_pattern <- " (#|Ste|SUITE|SHOP|UNIT).*$"
restaurants <-
restaurants %>%
mutate(street_only = str_trim(str_replace(street_only,
address_unit_pattern, replacement = "")))
restaurants %>% distinct(street_only) %>% head(11)
address_unit_pattern <- " (#|STE|SUITE|SHOP|UNIT).*$"
head(unique(restaurants$Inspection_Score), 15)
restaurants %>%
distinct(Business_ID, Date, Inspection_Score, street_only) %>%
filter(Inspection_Score > 45) %>%
count(street_only) %>%
arrange(desc(n)) %>%
head(n=5)
head(unique(restaurants$Violation_Description))
head(str_split_fixed(restaurants$Violation_Description, " - ", n = 2))
wikipedia_scrape <- c("Class of 2018: Senior Stories of Discovery, Learning and Serving\n\n\t\t\t\t\t\t\t", "[This]","Professor","is","not","so","great")
wikipedia_scrape
grepl("Class", wikipedia_scrape)    # Search for a word
grepl("class", wikipedia_scrape)  # grepl is case sensitive
wikipedia_scrape
gsub("\t", "", wikipedia_scrape)  # delete \t
gsub("\t|\n", "", wikipedia_scrape)     # delete \t or \n, no space needed
gsub("\[|]", "", wikipedia_scrape)    #Doesnt work because \ has a meaning
gsub("\\[|\\]", "", wikipedia_scrape)
grep("^[P]", wikipedia_scrape)    #Find a special character
wikipedia_scrape
grep("^[P]", wikipedia_scrape)    #Find a special character
wikipedia_scrape[grep("^[P]", wikipedia_scrape)]
wikipedia_scrape
rm(list = ls())
library(tidyverse)
library(rvest)
wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
Section_wikipedia <- html_node(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div[1]/table')
Section_wikipedia
health_rankings<-html_table(Section_wikipedia)
View(health_rankings)
ggplot(health_rankings) +
geom_histogram(aes(x = `Attainment of goals / Health / Level (DALE)`))
View(health_rankings)
wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
Section_wikipedia1 <- html_node(wikipedia_page, css = '#mw-content-text > div.mw-parser-output > table')
Section_wikipedia1
table_wiki1 <- html_table(Section_wikipedia1)
table_wiki1
Legault <- read_excel("legault_2258590544197963.xlsx")
rm(list = ls())
rm(list = ls())
#install.packages("tidystopwords")
library(tidyverse)
library(readxl)
library(lubridate)
library(summarytools)
library(gtsummary)
library(tidytext)
library(tidystopwords)  # Stop word
Legault <- read_excel("legault_2258590544197963.xlsx")
Legault <-
Legault %>%
mutate(Nom = "Legault")
GND <- read_excel("gnd_1389794314713133.xlsx")
GND <-
GND %>%
mutate(Nom = "Nadeau")
Jdm <- read_excel("jdm_1844588875635766.xlsx")
Jdm <-
Jdm %>%
mutate(Nom = "Journal de Montréal")
## Fusion des fichiers
observatoire <- bind_rows(Legault, GND, Jdm)
freq(observatoire$Nom)
View(observatoire)
View(Jdm)
fbcomments <- read_csv("FBComment_updateC.csv")
fbposts <- read_delim("FBPost_updateC.csv", delim = "\t")
View(fbposts)
rm(list = ls())
library(tidyverse)
library(rvest)
wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
View(wikipedia_page)
Section_wikipedia <- html_node(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div[1]/table')
health_rankings <- html_table(Section_wikipedia)
View(health_rankings)
Section_wikipedia1 <- html_node(wikipedia_page, css = '#mw-content-text > div.mw-parser-output > table')
table_wiki1 <- html_table(Section_wikipedia1)
View(table_wiki1)
Section_wikipedia2 <- html_node(wikipedia_page, css = '.headerSort , .jquery-tablesorter td')
View(health_rankings)
rm(list = ls())
#install.packages("rtweet")
library(tidyverse)
library(rtweet)
library(httpuv)
library(maps)
app_name <- "cssforafrica"
consumer_key <- "piMILL2EfDVHIDlwJlizUt6CY"
consumer_secret <- "xhYID0ORAUDNzpJC0Y1mupB7Hpv6dkGvpSqFFdMHpqy0pd1ZOi"
google_key <- "AIzaSyCthR_V7YJNduHuA7jrLunlQQxukFYyhs4"
create_token(app = app_name, consumer_key = consumer_key, consumer_secret = consumer_secret, set_renv = TRUE)
help(rtweet)
canada_tweet <- search_tweets(q = "#Canada", n = 1000, include_rts = FALSE)
View(canada_tweet)
View(canada_tweet)
head(canada_tweet$text)
names(canada_tweet)
cd_tweets <- search_tweets("canada",
"lang:en", geocode = lookup_coords("USA"),
n = 1000, type="recent", include_rts=FALSE)
View(canada_tweet)
cd_tweets <- search_tweets("canada",
"lang:fr", geocode = lookup_coords("USA"),
n = 1000, type="recent", include_rts=FALSE)
View(canada_tweet)
?get_timelines
sanders_tweets <- get_timelines(c("PMLegault"), n = 15)
View(sanders_tweets)
get_trends("Montreal")
sanders_tweets <- get_timelines(c("francoislegault"), n = 15)
View(sanders_tweets)
