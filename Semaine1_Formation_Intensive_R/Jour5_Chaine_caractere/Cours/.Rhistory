library(tidyverse)
library(summarytools)
restaurants <- read_csv("../Données/restaurants.csv",
col_types = "ccccccccnnccicccciccciD")
restaurants <- readRDS("../Données/restaurants.RData")
glimpse(restaurants)
head(restaurants$Zip_Code)
restaurants <-
restaurants %>%
mutate(ZIP_length = nchar(Zip_Code)) #%>%
#  count(ZIP_length)
freq(restaurants$ZIP_length)
restaurants <-
restaurants %>%
mutate(ZIP_5 = substr(Zip_Code, 1, 5),
ZIP_4 = substr(Zip_Code, 2, 4))
?substr
restaurants %>% distinct(ZIP_5) %>% head()
restaurants %>% distinct(ZIP_4) %>% head()
head(unique(restaurants$Address))
restaurants <-
restaurants %>%
mutate(mailing_address =
paste(Address, ", ", City, ", WA ", ZIP_5, sep = ""))
restaurants %>% distinct(mailing_address) %>% head()
rm(list = ls())
library(tidyverse)
library(summarytools)
#library(xaringanthemer)
restaurants <- read_csv("../Données/restaurants.csv",
col_types = "ccccccccnnccicccciccciD")
#restaurants <- readRDS("../Données/restaurants.RData")
glimpse(restaurants)
#saveRDS(restaurants, "restaurants.RData")
rm(list = ls())
library(tidyverse)
library(summarytools)
#library(xaringanthemer)
restaurants <- read_csv("../Donnees/restaurants.csv",
col_types = "ccccccccnnccicccciccciD")
#restaurants <- readRDS("../Données/restaurants.RData")
glimpse(restaurants)
#saveRDS(restaurants, "restaurants.RData")
head(restaurants$Zip_Code)
restaurants <-
restaurants %>%
mutate(ZIP_length = nchar(Zip_Code)) #%>%
#  count(ZIP_length)
freq(restaurants$ZIP_length)
library(tidyverse)
library(rvest)
wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
View(wikipedia_page)
Section_wikipedia <- html_nodes(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div[1]/table')
Section_wikipedia1 <- html_nodes(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div/table[2]')
#Section_wikipedia1 <- html_nodes(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div/table[2]')
head(Section_wikipedia)
health_rankings <- html_table(Section_wikipedia1)
View(health_rankings)
head(health_rankings[,(1:2)])
health_rankings <- html_table(Section_wikipedia)
head(health_rankings[,(1:2)])
View(health_rankings)
wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
wikipedia_page
Section_wikipedia2 <- html_node(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div/table[2]')
head(Section_wikipedia2)
health_rankings<-html_table(Section_wikipedia2)
head(health_rankings[,(1:2)])
Section_wikipedia <- html_nodes(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div[1]/table')
View(Section_wikipedia)
Section_wikipedia <- html_node(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div[1]/table')
health_rankings<-html_table(Section_wikipedia)
head(health_rankings[,(1:2)])
View(health_rankings)
socio1 <- read_html("https://sociologie.uqam.ca/corps-professoral/professeurs-es/")
socio1
socio_prof1 <- html_node(socio1, xpath = '//*[@id="post-967"]/div/div/div[1]/div[2]/div[1]')
socio_prof1
Section_wikipedia
socio_prof_list <- html_text(socio_prof1)
socio_prof_list
socio_prof1 <- html_node(socio1, xpath = '//*[@id="post-967"]/div/div')
socio_prof1
socio_prof_list <- html_text(socio_prof1)
socio_prof_list
wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
Section_wikipedia1 <- html_node(wikipedia_page, css = '.jquery-tablesorter td+ td , .flagicon+ a , .headerSort')
Section_wikipedia1
table_wiki1 <- html_table(Section_wikipedia3)
table_wiki1 <- html_table(Section_wikipedia1)
table_wiki1 <- html_text(Section_wikipedia1)
table_wiki1
Section_wikipedia1 <- html_node(wikipedia_page, css = '.jquery-tablesorter td , .headerSort')
Section_wikipedia1
table_wiki1 <- html_text(Section_wikipedia1)
Section_wikipedia1 <- html_node(wikipedia_page, css = '.jquery-tablesorter td')
wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
Section_wikipedia1 <- html_node(wikipedia_page, css = '.jquery-tablesorter td')
Section_wikipedia1 <- html_node(wikipedia_page, xpath = '.jquery-tablesorter td')
Section_wikipedia1 <- html_node(wikipedia_page, xpath = '.jquery-tablesorter td')
Section_wikipedia1 <- html_node(wikipedia_page, css = '.jquery-tablesorter td+ td , .flagicon+ a , .headerSort')
Section_wikipedia1
table_wiki1 <- html_text(Section_wikipedia1)
table_wiki1
Section_wikipedia1 <- html_node(wikipedia_page, css = '.headerSort , .jquery-tablesorter td')
#mw-content-text > div.mw-parser-output > table
#mw-content-text > div.mw-parser-output > table
Section_wikipedia1 <- html_node(wikipedia_page, css = '#mw-content-text > div.mw-parser-output > table')
table_wiki1 <- html_text(Section_wikipedia1)
table_wiki1
table_wiki1 <- html_table(Section_wikipedia1)
table_wiki1
#.jquery-tablesorter td , .headerSort
Section_wikipedia1 <- html_node(wikipedia_page, css = '.jquery-tablesorter td , .headerSort')
lego_section <- html_node(lego_film, css = '.primary_photo+ td a')
lego_film <- read_html("https://www.imdb.com/title/tt1490017/")
lego_section <- html_node(lego_film, css = '.primary_photo+ td a')
lego_section
Nom_acteur <- html_table(lego_section)
Nom_acteur <- html_text(lego_section)
head(Nom_acteur)
lego_section <- html_node(lego_film, css = '.odd td')
lego_section
Nom_acteur <- html_table(lego_section)
Nom_acteur <- html_text(lego_section)
head(Nom_acteur)
lego_section <- html_node(lego_film, css = '.primary_photo+ td a , .primary_photo+ td')
lego_section
Nom_acteur <- html_text(lego_section)
head(Nom_acteur)
acteurs <-
lego %>%
html_nodes(css = ".primary_photo+ td a")
lego <- read_html("https://www.imdb.com/title/tt1490017/")
acteurs <-
lego %>%
html_nodes(css = ".primary_photo+ td a")
acteurs <-
lego %>%
html_node(css = ".primary_photo+ td a")
acteurs <-
lego %>%
html_node(css = ".primary_photo+ td a")
acteurs <-
lego %>%
html_node(css = '.primary_photo+ td a')
acteurs
acteurs <-
lego %>%
html_node(css = '.primary_photo+ td')
acteurs <-
lego %>%
html_node(css = '.primary_photo+ td')
lego <- read_html("https://www.imdb.com/title/tt1490017/")
aacteurs <- html_node(lego, css = '.primary_photo+ td')
length(acteurs)
acteurs <- html_node(lego, css = '.primary_photo+ td')
length(acteurs)
acteurs[1:2]
# Collecter les noms
acteurs_nom <- html_text(acteurs, trim = TRUE)
# Collecter les liens des pages des acteurs
acteur_attr <- html_attrs(acteurs)
# Relative url
acteurs_rel_url <- html_attr(acteurs, "href")
acteurs <- html_node(lego, xpath = '.primary_photo+ td')
acteurs <- html_node(lego, xpath = '.primary_photo+ td')
acteurs <- html_nodes(lego, xpath = '.primary_photo+ td')
acteurs <- html_nodes(lego, xpath = '.primary_photo+ td a')
acteurs <-
lego %>%
html_node(css = '.primary_photo+ td a')
#.orange a span
thinkr_formateurs_nodes <- html_node(thinkr_url, css = '.orange , .orange span')
thinkr_url <- read_html("https://thinkr.fr/equipe/")
#.orange a span
thinkr_formateurs_nodes <- html_node(thinkr_url, css = '.orange , .orange span')
length(thinkr_formateurs_nodes)
thinkr_formateurs_nodes[1:3]
formateur <- html_text(thinkr_formateurs_nodes)
length(formateur)
formateur[3]
length(formateur)
#.orange a span
thinkr_formateurs_nodes <- html_node(thinkr_url, css = '.orange span')
length(thinkr_formateurs_nodes)
thinkr_formateurs_nodes[1:3]
formateur <- html_text(thinkr_formateurs_nodes)
length(formateur)
formateur[3]
formateur[1]
formateur[2]
sociouqam_url <- read_html("https://sociologie.uqam.ca/corps-professoral/professeurs-es/")
profs <-
sociouqam_url %>%
html_nodes('.nom a') %>%
html_text(trim = TRUE)
head(profs)
profs[1]
profs[2]
prof_table <- as_data_frame(c(profs[1:35]))
prof_table
nyt_nodes <-
nyt_url %>%
html_nodes('.css-aa7djq :nth-child(1)')
nyt_url <- read_html("https://www.nytimes.com/2019/11/28/opinion/thanksgiving-trump.html#commentsContainer")
nyt_nodes <-
nyt_url %>%
html_nodes('.css-aa7djq :nth-child(1)')
lenght(nyt_nodes)
nyt_nodes <-
nyt_url %>%
html_nodes('//*[@id="comment-content-1"]')
system('docker run -d -p 4445:4444 selenium/standalone-chrome')
#install.packages("RSelenium")
library(RSelenium)
# start a Selenium server
rD <- rsDriver(browser = c("chrome"), chromever = "78.0.3904.105")
# open the browser
remDr <- rD$client
remDr$navigate("https://www.duke.edu")
remDr$open("https://www.duke.edu)
remDr$open("https://www.duke.edu")
remDr$open()
remDr$navigate("https://www.duke.edu")
remDr$navigate("https://www.duke.edu")
search_box <- remDr$findElement(using = 'css selector', 'fieldset input')
search_box$sendKeysToElement(list("data science", "\uE007"))
remDr$close()
rD$server$stop()
# start a Selenium server
rD <- rsDriver(browser = c("chrome"), chromever = "78.0.3904.105")
# open the browser
remDr <- rD$client
# open the browser
remDr <- rD$client
remDr$navigate("https://www.duke.edu")
remDr$open()
rm(list = ls())
library(tidyverse)
#install.packages("tidystopwords")
library(tidyverse)
library(readxl)
library(lubridate)
library(summarytools)
library(gtsummary)
library(tidytext)
library(tidystopwords)  # Stop word
fbcomments <- read_csv("FBComment_updateC.csv")
fbposts <- read_csv("FBPost_updateC.csv")
fbposts <- read_csv("FBPost_updateC.csv")
fbposts <- read_delim("FBPost_updateC.csv", delim = "\t"))
fbposts <- read_delim("FBPost_updateC.csv", delim = "\t")
View(fbposts)
