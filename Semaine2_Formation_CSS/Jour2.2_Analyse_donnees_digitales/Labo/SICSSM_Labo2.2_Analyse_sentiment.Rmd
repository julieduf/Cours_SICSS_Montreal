---
title: 'Labo 2.2: Analyse quantitative du texte'
subtitle: 'Analyse de sentiments'
author: "Visseho Adjiwanou, PhD."
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

# Analyse de sentiments

L'analyse de sentiment est une technique d'extraction d'information dans un corpus. Le but est de détecter les mots cherchés/voulus dans le corpus. Pour cela, on va se baser sur les dictionnaires qui exsitent ou définir son propre dictionnaire.


```{r}

rm(list = ls())
#install.packages("tidytext")
#install.packages("textdata")

library(tidyverse)
library(tidytext)
library(textdata) # 
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)


```

Nous allons charger les données de trump tweet


```{r}

load(url("https://cbail.github.io/Trump_Tweets.Rdata")) #```{r} I activated this one and commented the following code. The trumptweets <- readRDS("trumptweets.RData") throws error.


trumptweets <- readRDS("../Données/trumptweets.RData")


tidy_trump_tweets <- 
  trumptweets %>% 
  select(created_at, text) %>% 
  unnest_tokens("word", text)     # Tokenise the data
  
tidy_trump_tweets  

head(tidy_trump_tweets, 12)

tidy_trump_tweets %>% 
  count(word) %>% 
  arrange(desc(n))

```

```{r}

data("stop_words")
# head(stopwords()) 
head(stop_words) 

tidy_trump_tweets <- 
  tidy_trump_tweets %>% 
  anti_join(stop_words)


head(tidy_trump_tweets, 20)

tidy_trump_tweets %>% 
  count(word) %>% 
  arrange(desc(n))
  
```

```{r}

#tidy_trump_tweets <-
#  tidy_trump_tweets[-grep("https|t.co|amp|rt", tidy_trump_tweets$word), ] 

tidy_trump_tweets <-
  tidy_trump_tweets %>% 
  filter(!grepl("https|t.co|amp|rt", word))

tidy_trump_tweets %>% 
  count(word) %>% 
  arrange(desc(n))



# Enlever les chiffres

tidy_trump_tweets <-
  tidy_trump_tweets %>% 
  filter(!grepl("\\b\\d+\\b", word))


#tidy_trump_tweets <- tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]

# OUne fois de plus, tidytext rend automatiquement tous les mots en minuscules.

# Enlever l'espace

tidy_trump_tweets <-
  tidy_trump_tweets %>% 
  mutate(word = gsub("\\s+","", word))

tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)    

```

```{r}

tidy_trump_tweets %>% 
  count(word, sort = TRUE)


tidy_trump_tweets %>%          
  count(word) %>% 
  arrange(desc(n))

# Sélectionner les 20 mots les plus importants

top_20 <- 
  tidy_trump_tweets %>% 
  count(word, sort = TRUE) 

top_20 <- top_20[1:20, ]    
  
top_20  



```

```{r}
library(ggplot2) # ```{r} I added the library function for ggplot.

ggplot(top_20) +
  geom_bar(aes(x = word, y = n, fill = word), stat = "identity") +
  theme_minimal() +
  theme(axis.text = element_text(angle = 90, hjust = 1)) +
  labs(x = "mot", y = "Nombre de fois que le mot apparait dans un tweet") +
  guides(fill = FALSE)

```

```{r}


tidy_trump_tfidf <- 
  trumptweets %>%
  select(created_at, text) %>%
  unnest_tokens("word", text) %>%
 # anti_join(stop_words) %>%
  count(word, created_at) %>%
  bind_tf_idf(word, created_at, n)


top_tfidf <- tidy_trump_tfidf %>%
  arrange(desc(n))

top_tfidf

head(top_tfidf$word)

top_tfidf <- 
  tidy_trump_tfidf %>% 
  arrange(desc(tf_idf))

top_tfidf

```


```{r}

#economic_dictionary <- c("economy","unemployment","trade","tariffs")

economic_dictionary <- "economy|unemployment|trade|tariffs|employment"

economic_dictionary1 <- "Togo|Cameroon"


```


```{r}

economic_tweets <- 
  trumptweets %>% 
  filter(str_detect(text, economic_dictionary))

head(economic_tweets$text)


```

```{r}

head(get_sentiments("afinn"), 24)

summary(get_sentiments("afinn"))

head(get_sentiments("bing"), 24)
summary(get_sentiments("bing"))

```

```{r}

dictionnaire <- get_sentiments("bing")

trump_tweet_sentiment <- 
  tidy_trump_tweets %>%
  inner_join(dictionnaire) %>%
  count(created_at, sentiment) 

               
head(trump_tweet_sentiment)

#head(tidy_trump_tweets)

ggplot(trump_tweet_sentiment, aes(x=created_at, y=n, color=sentiment))+
  geom_line(size=.5)+
  theme_minimal()+
  labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
  theme_bw()

```

```{r}

library(lubridate)

tidy_trump_tweets <-
  tidy_trump_tweets %>% 
  mutate(date = date(created_at))


# Utilisons un meilleur regroupement

trump_tweet_sentiment1 <- 
  tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>%
  count(date, sentiment) 

ggplot(trump_tweet_sentiment1, aes(x=date, y=n, color=sentiment))+
  geom_line(size=.5)+
  theme_minimal()+
  labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
  theme_bw()

```

```{r}

trump_sentiment_negatif <-
  tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>% 
  filter(sentiment=="negative") %>%
  count(date, sentiment)

trump_sentiment_negatif

trump_sentiment_positif <-
  tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>% 
  filter(sentiment=="positive") %>%
  count(date, sentiment)

trump_sentiment_positif

trump_sentiment <-
  tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>% 
  count(date, sentiment)

trump_sentiment 

```

```{r}

negatif <-
  ggplot(trump_sentiment_negatif) +
  geom_line(aes(x = date, y = n), color = "red") +
  labs(x = "Date", y = "Fréquence de mots négative dans les tweet de Trump")

negatif

```

```{r}

trump_approval <- read.csv("https://projects.fivethirtyeight.com/trump-approval-data/approval_topline.csv")

head(trump_approval)

trump_approval <-
  trump_approval %>% 
  mutate(date = mdy(modeldate))

head(trump_approval)


approval_plot <-
  trump_approval %>%
  filter(subgroup == "Adults") %>%
  #filter(date > min(trump_sentiment_plot$date)) %>% 
  group_by(date) %>%
  summarise(approval = mean(approve_estimate))

head(approval_plot)

# Graphique

approval <-
  ggplot(approval_plot)+
  geom_line(aes(x = date, y = approval))+
  #theme_minimal()+
  labs(x = "Date", y = "% des Américains qui aprouvent Trump")

approval
```

```{r}
library(ggpubr)

ggarrange(negatif, approval, nrow = 2)

```

```{r}

nrc <- get_sentiments("nrc")
nrc

trump_tweet_sentiment_nrc <-
  tidy_trump_tweets %>% 
  inner_join(nrc) %>% 
  count(date, sentiment)

trump_tweet_sentiment_nrc



# Roue des sentiments

tidy_trump_tweets_nrc1 <-
  tidy_trump_tweets %>%  
  inner_join(nrc) %>%
  group_by(sentiment) %>% 
  count() %>% 
  ungroup()%>% 
  arrange(desc(sentiment)) %>%
  mutate(percentage = round(n/sum(n),4)*100,
         lab.pos = cumsum(percentage)-.5*percentage)

ggplot(data = tidy_trump_tweets_nrc1, 
       aes(x = 2, y = percentage, fill = sentiment))+
  geom_bar(stat = "identity")+
  coord_polar("y", start = 200) +
  geom_text(aes(y = lab.pos, label = paste(percentage,"%", sep = "")), col = "white") +
  theme_void() +
  #scale_fill_brewer(palette = "Dark2")+
  xlim(.5, 2.5) +
  ggtitle("Sentiment dans les tweets de Trump")

```



Il existe de nombreux autres types d'analyse des sentiments, que nous n'avons pas le temps de couvrir ici. Cependant, vous devez savoir que différents outils d'analyse des sentiments fonctionnent mieux pour certains corpus que pour d'autres. Voici une figure d'un article récent qui applique une variété de dictionnaires de sentiments différents à différents corpus:

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/comparaison.png)
Source: http://homepages.dcc.ufmg.br/~fabricio/download/cosn127-goncalves.pdf


