load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
#load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
trumptweets <- readRDS("trumptweets.Rdata")
#load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
trumptweets <- readRDS("../Données/trumptweets.Rdata")
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)     # Tokenise the data
#install.packages("tidytext")
#install.packages("textdata")
library(tidyverse)
library(tidytext)
library(textdata)
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
library(topicmodels)
#load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
trumptweets <- readRDS("../Données/trumptweets.Rdata")
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)     # Tokenise the data
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets <-
tidy_trump_tweets[-grep("https|t.co|amp|rt", tidy_trump_tweets$word), ]
tidy_trump_tweets <- tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]
tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
trump_tweets_dtm <-
tidy_trump_tweets %>%
count(created_at, word) %>%
cast_dtm(created_at, word, n)
inspect(trump_tweets_dtm[1:5,1:8])
trump_tweet_lda <- LDA(trump_tweets_dtm, k = 3, control = list(seed = 3425))
trump_tweet_lda
tt_topics <- tidy(trump_tweet_lda, matrix = "beta")
View(tt_topics)
tt_topics
tt_top_term <-
tt_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, desc(beta))
tt_top_term
tt_top_term %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
data("AssociatedPress")
library(topicmodels)
data("AssociatedPress")
AssociatedPress
inspect(AssociatedPress[6:20, 10:25])
ap_lda <- LDA(AssociatedPress, k = 5, control = list(seed = 12345))
ap_lda
# Trouver les topics
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
ap_top_topic <-
ap_topics %>%
group_by(topic) %>%
top_n(10) %>%
arrange(topic, -beta)
ap_top_topic
ap_top_topic %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
library(tidyverse)
library(tidytext)
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
trumptweets <- readRDS("../Données/trumptweets.Rdata")
head(trumptweets)
trump_corpus <- Corpus(VectorSource(as.vector(trumptweets$text)))
trump_corpus
View((trump_corpus))
# Les informations de chaque document
trump_corpus[[2]][["content"]]
trump_corpus[[2]][["meta"]]
# Enlever les chiffres
trump_corpus <- tm_map(trump_corpus, content_transformer(removeNumbers))
# Transformer en minuscule
trump_corpus <- tm_map(trump_corpus,  content_transformer(tolower))
# Enlever les espaces
trump_corpus <- tm_map(trump_corpus, content_transformer(stripWhitespace))
# Stemming
trump_corpus  <- tm_map(trump_corpus, content_transformer(stemDocument), language = "english")
?DocumentTermMatrix()
trump_DTM <- DocumentTermMatrix(trump_corpus, control = list(wordLengths = c(3, Inf)))
inspect(trump_DTM[1:5,1:8])
trump_DTM_matrix <- as.matrix(trump_DTM)
trump_DTM_matrix[1:5,1:8]
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)     # Tokenise the data
tidy_trump_tweets
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
data("stop_words")
head(stopwords(), 15)
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
http_trump <- data.frame(word = c("https", "t.co", "amp", "rt"))
http_trump
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(http_trump)
tidy_trump_tweets  %>%
count(word) %>%
arrange(desc(n))
tidy_trump_tweets <- tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]
# Enlever les espaces
tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)
# Stemming
library(SnowballC)
tidy_trump_tweets<-tidy_trump_tweets %>%
mutate_at("word", funs(wordStem((.), language="en")))
View(tidy_trump_tweets)
tidy_trump_tweets %>%
count(word, sort = TRUE)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
top_20 <-
tidy_trump_tweets %>%
count(word, sort = TRUE)
top_20 <- top_20[1:20, ]
top_20
ggplot(top_20) +
geom_col(aes(x = word, y = n, fill = word)) +
theme_bw() +
theme(axis.text = element_text(angle = 90, hjust = 1)) +
ylab("Number of time a word appears in a tweet") +
xlab("word") +
guides(fill = FALSE)
ggplot(top_20) +
geom_col(aes(x = word, y = n, fill = word)) +
theme_bw() +
theme(axis.text = element_text(angle = 90, hjust = 1)) +
ylab("Number of time a word appears in a tweet") +
xlab("word") +
guides(fill = FALSE) %>%
coord_flip()
ggplot(top_20) +
geom_col(aes(x = word, y = n, fill = word)) +
theme_bw() +
theme(axis.text = element_text(angle = 90, hjust = 1)) +
ylab("Number of time a word appears in a tweet") +
xlab("word") +
guides(fill = FALSE)
tidy_trump_tweets_tfidf <-
tidy_trump_tweets %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n)
top_tfidf <-
tidy_trump_tweets_tfidf %>%
arrange(desc(tf_idf))
top_tfidf
tidy_trump_tfidf<- trumptweets %>%
select(created_at,text) %>%
unnest_tokens("word", text) %>%
anti_join(stop_words) %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n) %>%
filter(tf_idf < 4)
top_tfidf <- tidy_trump_tfidf %>%
arrange(desc(tf_idf))
top_tfidf$word[1]
dtm_trumptweets <-
tidy_trump_tweets %>%
count(created_at, word) #%>%
dtm_trumptweets <-
tidy_trump_tweets %>%
count(created_at, word) %>%
cast_dtm(created_at, word, n)
dtm_trumptweets
inspect(dtm_trumptweets[1:5,1:8])
dtm_trumptweets %>% {
wordcloud(.$word, .$n, max.words = 20)
}
inspect(dtm_trumptweets[1:5,1:8])
trump_DTM %>% {
wordcloud(.$word, .$n, max.words = 20)
}
?wordcloud()
wordcloud(dtm_trumptweets$word, dtm_trumptweets$n, max.words = 20)
dtm_trumptweets_matrix <- as.matrix(dtm_trumptweets)
dtm_trumptweets_matrix %>% {
wordcloud(.$word, .$n, max.words = 20)
}
dtm_trumptweets_matrix[1:5,1:8]
wordcloud(dtm_trumptweets$word, dtm_trumptweets$n)
wordcloud(dtm_trumptweets_matrix$word, dtm_trumptweets_matrix$n)
wordcloud(dtm_trumptweets_matrix$Terms, dtm_trumptweets_matrix$n)
wordcloud(dtm_trumptweets_matrix[, Terms], dtm_trumptweets_matrix$n)
wordcloud(dtm_trumptweets_matrix[, Terms], dtm_trumptweets_matrix[, n])
wordcloud(dtm_trumptweets_matrix[, Terms], dtm_trumptweets_matrix[n, ])
rm(list = ls())
#install.packages("tidytext")
#install.packages("textdata")
library(tidyverse)
library(tidytext)
library(textdata) #
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
load(url("https://cbail.github.io/Trump_Tweets.Rdata")) #```{r} I activated this one and commented the following code. The trumptweets <- readRDS("trumptweets.RData") throws error.
trumptweets <- readRDS("../Données/trumptweets.RData")
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)     # Tokenise the data
tidy_trump_tweets
head(tidy_trump_tweets, 12)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
data("stop_words")
# head(stopwords())
head(stop_words)
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
head(tidy_trump_tweets, 20)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
tidy_trump_tweets <-
tidy_trump_tweets %>%
filter(!grepl("https|t.co|amp|rt", word))
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
tidy_trump_tweets <-
tidy_trump_tweets %>%
filter(!grepl("\\b\\d+\\b", word))
tidy_trump_tweets <-
tidy_trump_tweets %>%
filter(!grepl("https|t.co|amp|rt", word))
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
tidy_trump_tweets <-
tidy_trump_tweets %>%
filter(!grepl("\\b\\d+\\b", word))
tidy_trump_tweets <-
tidy_trump_tweets %>%
mutate(word = gsub("\\s+","", word))
tidy_trump_tweets <-
tidy_trump_tweets %>%
mutate(word = gsub("\\s+","", word))
tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)
```{r}
tidy_trump_tweets %>%
count(word, sort = TRUE)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
top_20 <-
tidy_trump_tweets %>%
count(word, sort = TRUE)
top_20 <- top_20[1:20, ]
top_20
ggplot(top_20) +
geom_bar(aes(x = word, y = n, fill = word), stat = "identity") +
theme_minimal() +
theme(axis.text = element_text(angle = 90, hjust = 1)) +
labs(x = "mot", y = "Nombre de fois que le mot apparait dans un tweet") +
guides(fill = FALSE)
tidy_trump_tfidf <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text) %>%
# anti_join(stop_words) %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n)
tidy_trump_tfidf <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text) %>%
# anti_join(stop_words) %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n)
top_tfidf <- tidy_trump_tfidf %>%
arrange(desc(n))
top_tfidf <- tidy_trump_tfidf %>%
arrange(desc(n))
top_tfidf
head(top_tfidf$word)
top_tfidf <-
tidy_trump_tfidf %>%
arrange(desc(tf_idf))
top_tfidf
economic_tweets <-
trumptweets %>%
filter(str_detect(text, economic_dictionary))
economic_dictionary <- "economy|unemployment|trade|tariffs|employment"
economic_dictionary1 <- "Togo|Cameroon"
economic_tweets <-
trumptweets %>%
filter(str_detect(text, economic_dictionary))
head(economic_tweets$text)
head(get_sentiments("afinn"), 24)
summary(get_sentiments("afinn"))
head(get_sentiments("bing"), 24)
summary(get_sentiments("bing"))
dictionnaire <- get_sentiments("bing")
trump_tweet_sentiment <-
tidy_trump_tweets %>%
inner_join(dictionnaire) %>%
count(created_at, sentiment)
View(tidy_trump_tweets)
head(trump_tweet_sentiment)
ggplot(trump_tweet_sentiment) +
geom_line(aes(x=created_at, y=n, color=sentiment), size=.5) +
theme_minimal()+
labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
theme_bw()
library(lubridate)
tidy_trump_tweets <-
tidy_trump_tweets %>%
mutate(date = date(created_at))
trump_tweet_sentiment1 <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
count(date, sentiment)
ggplot(trump_tweet_sentiment1) +
geom_line(aes(x=date, y=n, color=sentiment), size=.5) +
theme_minimal() +
labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
theme_bw()
trump_sentiment_negatif <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
filter(sentiment=="negative") %>%
count(date, sentiment)
trump_sentiment_negatif
trump_sentiment_positif <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
filter(sentiment=="positive") %>%
count(date, sentiment)
trump_sentiment_positif
trump_sentiment <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
count(date, sentiment)
trump_sentiment
negatif <-
ggplot(trump_sentiment_negatif) +
geom_line(aes(x = date, y = n), color = "red") +
labs(x = "Date", y = "Fréquence de mots négative dans les tweet de Trump")
negatif
trump_approval <- read.csv("https://projects.fivethirtyeight.com/trump-approval-data/approval_topline.csv")
head(trump_approval)
trump_approval <-
trump_approval %>%
mutate(date = mdy(modeldate))
head(trump_approval)
approval_plot <-
trump_approval %>%
filter(subgroup == "Adults") %>%
#filter(date > min(trump_sentiment_plot$date)) %>%
group_by(date) %>%
summarise(approval = mean(approve_estimate))
head(approval_plot)
approval <-
ggplot(approval_plot) +
geom_line(aes(x = date, y = approval)) +
#theme_minimal()+
labs(x = "Date", y = "% des Américains qui aprouvent Trump")
approval
library(ggpubr)
ggarrange(negatif, approval, nrow = 2)
nrc <- get_sentiments("nrc")
trump_tweet_sentiment_nrc <-
tidy_trump_tweets %>%
inner_join(nrc) %>%
count(date, sentiment)
trump_tweet_sentiment_nrc
tidy_trump_tweets_nrc1 <-
tidy_trump_tweets %>%
inner_join(nrc) %>%
group_by(sentiment) %>%
count() %>%
ungroup()%>%
arrange(desc(sentiment)) %>%
mutate(percentage = round(n/sum(n), 4)*100,
lab.pos = cumsum(percentage)-.5*percentage)
View(tidy_trump_tweets_nrc1)
tidy_trump_tweets_nrc1 <-
tidy_trump_tweets %>%
inner_join(nrc) %>%
group_by(sentiment) %>%
count() %>%
ungroup()%>%
arrange(desc(sentiment)) %>%
mutate(percentage = round(n/sum(n), 4)*100,
lab.pos = cumsum(percentage))
ggplot(data = tidy_trump_tweets_nrc1, aes(x = 2, y = percentage, fill = sentiment))+
geom_bar(stat = "identity")+
coord_polar("y", start = 200) +
geom_text(aes(y = lab.pos, label = paste(percentage,"%", sep = "")), col = "white") +
theme_void() +
#scale_fill_brewer(palette = "Dark2")+
xlim(.5, 2.5) +
ggtitle("Sentiment dans les tweets de Trump")
tidy_trump_tweets_nrc1 <-
tidy_trump_tweets %>%
inner_join(nrc) %>%
group_by(sentiment) %>%
count() %>%
ungroup()%>%
arrange(desc(sentiment)) %>%
mutate(percentage = round(n/sum(n), 4)*100,
lab.pos = cumsum(percentage)-.5*percentage)
ggplot(data = tidy_trump_tweets_nrc1, aes(x = 2, y = percentage, fill = sentiment))+
geom_bar(stat = "identity")+
coord_polar("y", start = 200) +
geom_text(aes(y = lab.pos, label = paste(percentage,"%", sep = "")), col = "white") +
theme_void() +
#scale_fill_brewer(palette = "Dark2")+
xlim(.5, 2.5) +
ggtitle("Sentiment dans les tweets de Trump")
# Effacer l'environnement
rm(list = ls())
# Installer les différents packages
# Charger le packages
library(tidyverse)
library(lubridate)
library(stringr)
library(forcats)
library(modelr)
library(tidytext)
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
DIR_SOURCE <- system.file("extdata/federalist", package = "qss")
corpus_raw <- VCorpus(DirSource(directory = DIR_SOURCE, pattern = "fp"))
corpus_raw
